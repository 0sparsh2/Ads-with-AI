{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7899426,"sourceType":"datasetVersion","datasetId":4638922}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-20T21:49:37.094375Z","iopub.execute_input":"2024-03-20T21:49:37.094764Z","iopub.status.idle":"2024-03-20T21:49:37.470989Z","shell.execute_reply.started":"2024-03-20T21:49:37.094702Z","shell.execute_reply":"2024-03-20T21:49:37.469775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install peft\n#!pip install trl\n!pip install bitsandbytes\n!pip install -i https://pypi.org/simple/ bitsandbytes\n!pip install accelerate","metadata":{"execution":{"iopub.status.busy":"2024-03-20T21:49:39.518878Z","iopub.execute_input":"2024-03-20T21:49:39.519664Z","iopub.status.idle":"2024-03-20T21:50:27.832727Z","shell.execute_reply.started":"2024-03-20T21:49:39.519632Z","shell.execute_reply":"2024-03-20T21:50:27.831737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport os\nimport sys\nimport json\nimport IPython\nfrom datetime import datetime\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    AutoTokenizer,\n    TrainingArguments,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T21:50:52.582161Z","iopub.execute_input":"2024-03-20T21:50:52.582475Z","iopub.status.idle":"2024-03-20T21:50:56.391781Z","shell.execute_reply.started":"2024-03-20T21:50:52.582447Z","shell.execute_reply":"2024-03-20T21:50:56.390913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install -U datasets","metadata":{"execution":{"iopub.status.busy":"2024-03-20T21:50:27.834795Z","iopub.execute_input":"2024-03-20T21:50:27.835135Z","iopub.status.idle":"2024-03-20T21:50:40.290353Z","shell.execute_reply.started":"2024-03-20T21:50:27.835105Z","shell.execute_reply":"2024-03-20T21:50:40.288999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install trl","metadata":{"execution":{"iopub.status.busy":"2024-03-20T21:50:40.291923Z","iopub.execute_input":"2024-03-20T21:50:40.292278Z","iopub.status.idle":"2024-03-20T21:50:52.579768Z","shell.execute_reply.started":"2024-03-20T21:50:40.292243Z","shell.execute_reply":"2024-03-20T21:50:52.578771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2024-03-20T21:51:08.357482Z","iopub.execute_input":"2024-03-20T21:51:08.358172Z","iopub.status.idle":"2024-03-20T21:51:08.396557Z","shell.execute_reply.started":"2024-03-20T21:51:08.358137Z","shell.execute_reply":"2024-03-20T21:51:08.395826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from accelerate import FullyShardedDataParallelPlugin, Accelerator\nfrom torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n\nfsdp_plugin = FullyShardedDataParallelPlugin(\n    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n)\n\naccelerator = Accelerator(fsdp_plugin=fsdp_plugin)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T21:51:10.728904Z","iopub.execute_input":"2024-03-20T21:51:10.729877Z","iopub.status.idle":"2024-03-20T21:51:10.758448Z","shell.execute_reply.started":"2024-03-20T21:51:10.729842Z","shell.execute_reply":"2024-03-20T21:51:10.757203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Chose the base model you want\nmodel_name = \"Hugofernandez/Mistral-7B-v0.1-colab-sharded\"\n# set device\ndevice = 'cuda'\n#v Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n# We redefine the pad_token and pad_token_id with out of vocabulary token (unk_token)\ntokenizer.pad_token = tokenizer.unk_token\ntokenizer.pad_token_id = tokenizer.unk_token_id","metadata":{"execution":{"iopub.status.busy":"2024-03-20T21:51:12.972333Z","iopub.execute_input":"2024-03-20T21:51:12.972745Z","iopub.status.idle":"2024-03-20T21:51:13.164294Z","shell.execute_reply.started":"2024-03-20T21:51:12.972688Z","shell.execute_reply":"2024-03-20T21:51:13.163473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Quantization as defined https://huggingface.co/docs/optimum/concept_guides/quantization will help us reduce the size of the model for it to fit on a single GPU\n#Quantization configuration\ncompute_dtype = getattr(torch, \"float16\")\nprint(compute_dtype)\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T21:51:14.091342Z","iopub.execute_input":"2024-03-20T21:51:14.091748Z","iopub.status.idle":"2024-03-20T21:51:14.099052Z","shell.execute_reply.started":"2024-03-20T21:51:14.091681Z","shell.execute_reply":"2024-03-20T21:51:14.098016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install -i https://pypi.org/simple/ bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-03-20T21:51:15.418886Z","iopub.execute_input":"2024-03-20T21:51:15.419257Z","iopub.status.idle":"2024-03-20T21:51:27.566866Z","shell.execute_reply.started":"2024-03-20T21:51:15.419226Z","shell.execute_reply":"2024-03-20T21:51:27.565534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load the model and quantize it\nmodel = AutoModelForCausalLM.from_pretrained(\n          model_name,\n          quantization_config=bnb_config,\n          use_flash_attention_2 = False, #set to True you're using A100\n          device_map={\"\": 0}, #device_map=\"auto\" will cause a problem in the training\n\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T21:51:27.569036Z","iopub.execute_input":"2024-03-20T21:51:27.569388Z","iopub.status.idle":"2024-03-20T21:56:03.690579Z","shell.execute_reply.started":"2024-03-20T21:51:27.569356Z","shell.execute_reply":"2024-03-20T21:56:03.689758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)\n#You can see that all the layers are Linear4bit","metadata":{"execution":{"iopub.status.busy":"2024-03-20T21:56:03.692229Z","iopub.execute_input":"2024-03-20T21:56:03.692541Z","iopub.status.idle":"2024-03-20T21:56:03.699689Z","shell.execute_reply.started":"2024-03-20T21:56:03.692513Z","shell.execute_reply":"2024-03-20T21:56:03.698684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peft_config = LoraConfig(\n        lora_alpha=16,\n        lora_dropout=0.05,\n        r=16,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\", \"lm_head\",]\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T21:56:03.700628Z","iopub.execute_input":"2024-03-20T21:56:03.700915Z","iopub.status.idle":"2024-03-20T21:56:03.710657Z","shell.execute_reply.started":"2024-03-20T21:56:03.700892Z","shell.execute_reply":"2024-03-20T21:56:03.709718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Cast some modules of the model to fp32\nmodel = prepare_model_for_kbit_training(model)\n#Configure the pad token in the model\nmodel.config.pad_token_id = tokenizer.pad_token_id\nmodel.config.use_cache = False # Gradient checkpointing is used by default but not compatible with caching","metadata":{"execution":{"iopub.status.busy":"2024-03-20T21:57:50.450939Z","iopub.execute_input":"2024-03-20T21:57:50.451883Z","iopub.status.idle":"2024-03-20T21:57:50.483272Z","shell.execute_reply.started":"2024-03-20T21:57:50.451848Z","shell.execute_reply":"2024-03-20T21:57:50.482423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n        output_dir=\"./results\", # directory in which the checkpoint will be saved.\n        evaluation_strategy=\"epoch\", # you can set it to 'steps' to eval it every eval_steps\n        optim=\"paged_adamw_8bit\", #used with QLoRA\n        per_device_train_batch_size=4, #batch size\n        per_device_eval_batch_size=4, #same but for evaluation\n        gradient_accumulation_steps=1, #number of lines to accumulate gradient, carefull because it changes the size of a \"step\".Therefore, logging, evaluation, save will be conducted every gradient_accumulation_steps * xxx_step training example\n        log_level=\"debug\", #you can set it to  ‘info’, ‘warning’, ‘error’ and ‘critical’\n        save_steps=500, #number of steps between checkpoints\n        logging_steps=20, #number of steps between logging of the loss for monitoring adapt it to your dataset size\n        learning_rate=4e-4, #you can try different value for this hyperparameter\n        num_train_epochs=1,\n        warmup_steps=100,\n        lr_scheduler_type=\"constant\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T21:57:53.262357Z","iopub.execute_input":"2024-03-20T21:57:53.263179Z","iopub.status.idle":"2024-03-20T21:57:53.272920Z","shell.execute_reply.started":"2024-03-20T21:57:53.263149Z","shell.execute_reply":"2024-03-20T21:57:53.272087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Define the root directory\nroot_dir = '/kaggle/input'\n\n# Loop through directories and files\nfor dirname, _, filenames in os.walk(os.path.join(root_dir, 'company-slogans')):\n    # Create the \"dataset\" folder within \"company-slogans\" if it doesn't exist\n    dataset_folder = os.path.join(dirname, 'dataset')\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T21:58:58.729789Z","iopub.execute_input":"2024-03-20T21:58:58.730848Z","iopub.status.idle":"2024-03-20T21:58:58.748561Z","shell.execute_reply.started":"2024-03-20T21:58:58.730804Z","shell.execute_reply":"2024-03-20T21:58:58.747518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef split_csv(input_file, train_output_file, test_output_file, test_size=0.2, random_state=None):\n    # Read the CSV file into a DataFrame\n    data = pd.read_csv(input_file)\n    data = data.dropna()\n    data['combined'] = '### Product: ' + data['company'] + ' \\n### Slogan: ' + data['slogan']\n\n    # Split the data into training and testing sets\n    train_data, test_data = train_test_split(data, test_size=test_size, random_state=random_state)\n\n    # Write the training and testing sets to separate CSV files\n    train_data.to_csv(train_output_file, index=False)\n    test_data.to_csv(test_output_file, index=False)\n\n# Example usage:\ninput_file = '/kaggle/input/company-slogans/slogans.csv'  # Replace 'data.csv' with the path to your CSV file\ntrain_output_file = 'train.csv'\ntest_output_file = 'test.csv'\nsplit_csv(input_file, train_output_file, test_output_file, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T05:17:37.918590Z","iopub.execute_input":"2024-03-21T05:17:37.919036Z","iopub.status.idle":"2024-03-21T05:17:38.036352Z","shell.execute_reply.started":"2024-03-21T05:17:37.919005Z","shell.execute_reply":"2024-03-21T05:17:38.035140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['test']['combined']","metadata":{"execution":{"iopub.status.busy":"2024-03-21T06:35:33.364314Z","iopub.execute_input":"2024-03-21T06:35:33.364700Z","iopub.status.idle":"2024-03-21T06:35:33.401957Z","shell.execute_reply.started":"2024-03-21T06:35:33.364669Z","shell.execute_reply":"2024-03-21T06:35:33.400670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First import your own dataset in the default folder which \"content\" on colab\n# The dataset should have one column named \"text\" with one example per line\ndata_files = {'train': \"/kaggle/working/train.csv\", 'test': \"/kaggle/working/test.csv\"}\ndataset = load_dataset('csv', data_files=data_files)\n\n#dataset = load_dataset(\"json\", data_files=\"path/to/dataset.jsonl\", split=\"train\")\n\n# Verify the chat template and apply it to you data\n# tokenizer.apply_chat_template(chat, tokenize=False)\n# Otherwise you can use dataset that are present on https://huggingface.co/datasets\n# dataset = load_dataset({DATASET_PATH})","metadata":{"execution":{"iopub.status.busy":"2024-03-21T05:17:39.625853Z","iopub.execute_input":"2024-03-21T05:17:39.627074Z","iopub.status.idle":"2024-03-21T05:17:40.028811Z","shell.execute_reply.started":"2024-03-21T05:17:39.627032Z","shell.execute_reply":"2024-03-21T05:17:40.027773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n        model=model,\n        train_dataset=dataset['train'],\n        eval_dataset=dataset['test'],\n        peft_config=peft_config,\n        dataset_text_field=\"combined\",\n        #packing = True\n        max_seq_length=1024,\n        tokenizer=tokenizer,\n        args=training_arguments,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T05:17:43.682703Z","iopub.execute_input":"2024-03-21T05:17:43.683447Z","iopub.status.idle":"2024-03-21T05:17:44.878641Z","shell.execute_reply.started":"2024-03-21T05:17:43.683414Z","shell.execute_reply":"2024-03-21T05:17:44.877436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = model.num_parameters()\n    for _, param in model.named_parameters():\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\nprint_trainable_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T05:17:47.685452Z","iopub.execute_input":"2024-03-21T05:17:47.685863Z","iopub.status.idle":"2024-03-21T05:17:47.722365Z","shell.execute_reply.started":"2024-03-21T05:17:47.685830Z","shell.execute_reply":"2024-03-21T05:17:47.720442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run an evaluation step\ntrainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-03-21T05:17:49.979583Z","iopub.execute_input":"2024-03-21T05:17:49.980593Z","iopub.status.idle":"2024-03-21T05:22:52.884815Z","shell.execute_reply.started":"2024-03-21T05:17:49.980557Z","shell.execute_reply":"2024-03-21T05:22:52.883797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Launch the training\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-21T05:22:52.887072Z","iopub.execute_input":"2024-03-21T05:22:52.887426Z","iopub.status.idle":"2024-03-21T06:31:07.006732Z","shell.execute_reply.started":"2024-03-21T05:22:52.887393Z","shell.execute_reply":"2024-03-21T06:31:07.005670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#trainer.evaluate()\n#eval_prompt = \"\"\"<s>[INST]What is a Neural Network, and how does it work?[/INST]\"\"\"\n\neval_prompt = \"### Product: Mercedes Car \\n### Slogan: The\"\n\n# import random\nmodel_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n\nmodel.eval()\nwith torch.no_grad():\n    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=64, pad_token_id=2)[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-03-21T06:43:41.545863Z","iopub.execute_input":"2024-03-21T06:43:41.546850Z","iopub.status.idle":"2024-03-21T06:43:49.343566Z","shell.execute_reply.started":"2024-03-21T06:43:41.546810Z","shell.execute_reply":"2024-03-21T06:43:49.342521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model = 'MistralAI_QLoRa_Ads'\ntrainer.model.save_pretrained(new_model)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T04:00:40.645057Z","iopub.execute_input":"2024-03-21T04:00:40.645432Z","iopub.status.idle":"2024-03-21T04:00:42.449653Z","shell.execute_reply.started":"2024-03-21T04:00:40.645392Z","shell.execute_reply":"2024-03-21T04:00:42.448525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load the base model\nbase_model = AutoModelForCausalLM.from_pretrained(model_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peft_model = PeftModel.from_pretrained(base_model, new_model)\nmerged_model = peft_model.merge_and_unload()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_merged_dir = \"/content/MistralAI_finetuned_Ads\"\n\nos.makedirs(output_merged_dir, exist_ok=True)\nmerged_model.save_pretrained(output_merged_dir, safe_serialization = False)\ntokenizer.save_pretrained(output_merged_dir)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import locale\nlocale.getpreferredencoding = lambda: \"UTF-8\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login\n\nmerged_model.push_to_hub(\"0sparsh2/MistralAI_finetuned_Ads\", check_pr=True)\n\ntokenizer.push_to_hub(\"0sparsh2/MistralAI_finetuned_Ads\",check_pr=True)\n","metadata":{},"execution_count":null,"outputs":[]}]}